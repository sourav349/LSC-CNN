{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "675f01ae-b815-4fca-af6d-b552ed969dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# -----------------------------\n",
    "# Residual Block with 128 Filters\n",
    "# -----------------------------\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, channels=128):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        return self.relu(out + residual)\n",
    "\n",
    "# -----------------------------\n",
    "# LSC_CNN Model\n",
    "# -----------------------------\n",
    "class LSC_CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Initial convolution layer\n",
    "        self.conv1 = nn.Conv2d(1, 128, kernel_size=7, padding=3)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "\n",
    "        # Intermediate convolution layers\n",
    "        self.conv2 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(128, 128, kernel_size=3, stride=2, padding=1)\n",
    "        self.relu3 = nn.ReLU(inplace=True)\n",
    "\n",
    "        # Residual blocks with 128 channels\n",
    "        self.res_blocks = nn.Sequential(*[ResidualBlock(128) for _ in range(20)])\n",
    "\n",
    "        # Upsampling and final refinement layers\n",
    "        self.conv21 = nn.Conv2d(1 + 128, 64, kernel_size=3, padding=1)\n",
    "        self.relu21 = nn.ReLU(inplace=True)\n",
    "        self.conv22 = nn.Conv2d(64, 1, kernel_size=3, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        inp = x  # Save the original input\n",
    "\n",
    "        # Encoding\n",
    "        x = self.relu1(self.conv1(x))\n",
    "        x = self.relu2(self.conv2(x))\n",
    "        x = self.relu3(self.conv3(x))\n",
    "\n",
    "        # Residual feature extraction\n",
    "        x = self.res_blocks(x)\n",
    "\n",
    "        # Upsampling to match input size\n",
    "        up = F.interpolate(x, scale_factor=2, mode='bilinear', align_corners=False)\n",
    "\n",
    "        # Match size with original input\n",
    "        h = min(inp.shape[2], up.shape[2])\n",
    "        w = min(inp.shape[3], up.shape[3])\n",
    "        inp = inp[:, :, :h, :w]\n",
    "        up = up[:, :, :h, :w]\n",
    "\n",
    "        # Element-wise multiplication and concatenation\n",
    "        enhanced = up * inp\n",
    "        concat = torch.cat((inp, enhanced), dim=1)\n",
    "\n",
    "        # Refinement and noise prediction\n",
    "        x = self.relu21(self.conv21(concat))\n",
    "        noise = self.conv22(x)\n",
    "\n",
    "        # Final clean image\n",
    "        return inp - noise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aef6cacc-9c90-4de4-b327-7d14740f2377",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset.py\n",
    "import os\n",
    "from PIL import Image\n",
    "from torchvision import transforms as T\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "\n",
    "class PairedIRDataset(Dataset):\n",
    "    def __init__(self, clean_dir, noisy_dir, crop_size=50):\n",
    "        self.clean_dir = clean_dir\n",
    "        self.noisy_dir = noisy_dir\n",
    "        self.filenames = sorted([\n",
    "            f for f in os.listdir(clean_dir)\n",
    "            if f.lower().endswith(('.jpg', '.png', '.bmp'))\n",
    "        ])\n",
    "        self.crop_size = crop_size\n",
    "        self.transform = T.ToTensor()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        fname = self.filenames[idx]\n",
    "        clean = Image.open(os.path.join(self.clean_dir, fname)).convert(\"L\")\n",
    "        noisy = Image.open(os.path.join(self.noisy_dir, fname.replace(\"Original\", \"\"))).convert(\"L\")\n",
    "\n",
    "        clean = self.transform(clean)\n",
    "        noisy = self.transform(noisy)\n",
    "\n",
    "        _, h, w = clean.shape\n",
    "        top = torch.randint(0, h - self.crop_size + 1, (1,)).item()\n",
    "        left = torch.randint(0, w - self.crop_size + 1, (1,)).item()\n",
    "        clean = clean[:, top:top+self.crop_size, left:left+self.crop_size]\n",
    "        noisy = noisy[:, top:top+self.crop_size, left:left+self.crop_size]\n",
    "\n",
    "        return noisy, clean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "778a4afc-c256-494f-908b-8bb7925a2203",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/himanshu/.local/lib/python3.10/site-packages/torch/nn/modules/conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at /opt/pytorch/aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
      "  return F.conv2d(input, weight, bias, self.stride,\n",
      "/home/himanshu/.local/lib/python3.10/site-packages/torch/autograd/graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at /opt/pytorch/aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Best model saved at epoch 1 ‚Äî Val Loss: 0.002233\n",
      "Epoch 1/25 | Train Loss: 0.052296 | Val Loss: 0.002233 | PSNR: 26.70 | SSIM: 0.6183\n",
      "‚úÖ Best model saved at epoch 2 ‚Äî Val Loss: 0.001578\n",
      "Epoch 2/25 | Train Loss: 0.001871 | Val Loss: 0.001578 | PSNR: 28.27 | SSIM: 0.7211\n",
      "‚úÖ Best model saved at epoch 3 ‚Äî Val Loss: 0.001461\n",
      "Epoch 3/25 | Train Loss: 0.001578 | Val Loss: 0.001461 | PSNR: 28.71 | SSIM: 0.7502\n",
      "Epoch 4/25 | Train Loss: 0.001423 | Val Loss: 0.001814 | PSNR: 28.18 | SSIM: 0.7383\n",
      "‚úÖ Best model saved at epoch 5 ‚Äî Val Loss: 0.001328\n",
      "Epoch 5/25 | Train Loss: 0.001349 | Val Loss: 0.001328 | PSNR: 29.26 | SSIM: 0.7912\n",
      "‚úÖ Best model saved at epoch 6 ‚Äî Val Loss: 0.001212\n",
      "Epoch 6/25 | Train Loss: 0.001286 | Val Loss: 0.001212 | PSNR: 29.55 | SSIM: 0.7927\n",
      "‚úÖ Best model saved at epoch 7 ‚Äî Val Loss: 0.001125\n",
      "Epoch 7/25 | Train Loss: 0.001246 | Val Loss: 0.001125 | PSNR: 30.01 | SSIM: 0.8209\n",
      "‚úÖ Best model saved at epoch 8 ‚Äî Val Loss: 0.001100\n",
      "Epoch 8/25 | Train Loss: 0.001214 | Val Loss: 0.001100 | PSNR: 30.12 | SSIM: 0.8194\n",
      "‚úÖ Best model saved at epoch 9 ‚Äî Val Loss: 0.001085\n",
      "Epoch 9/25 | Train Loss: 0.001151 | Val Loss: 0.001085 | PSNR: 30.16 | SSIM: 0.8389\n",
      "Epoch 10/25 | Train Loss: 0.001135 | Val Loss: 0.001144 | PSNR: 29.90 | SSIM: 0.8021\n",
      "Epoch 11/25 | Train Loss: 0.001099 | Val Loss: 0.001154 | PSNR: 29.76 | SSIM: 0.7874\n",
      "‚úÖ Best model saved at epoch 12 ‚Äî Val Loss: 0.001051\n",
      "Epoch 12/25 | Train Loss: 0.001095 | Val Loss: 0.001051 | PSNR: 30.38 | SSIM: 0.8480\n",
      "‚úÖ Best model saved at epoch 13 ‚Äî Val Loss: 0.001018\n",
      "Epoch 13/25 | Train Loss: 0.001055 | Val Loss: 0.001018 | PSNR: 30.54 | SSIM: 0.8576\n",
      "Epoch 14/25 | Train Loss: 0.001065 | Val Loss: 0.001048 | PSNR: 30.20 | SSIM: 0.7958\n",
      "Epoch 15/25 | Train Loss: 0.001036 | Val Loss: 0.001075 | PSNR: 30.26 | SSIM: 0.8231\n",
      "Epoch 16/25 | Train Loss: 0.001020 | Val Loss: 0.001019 | PSNR: 30.43 | SSIM: 0.8347\n",
      "‚úÖ Best model saved at epoch 17 ‚Äî Val Loss: 0.000952\n",
      "Epoch 17/25 | Train Loss: 0.000988 | Val Loss: 0.000952 | PSNR: 30.75 | SSIM: 0.8408\n",
      "Epoch 18/25 | Train Loss: 0.001002 | Val Loss: 0.000995 | PSNR: 30.46 | SSIM: 0.8242\n",
      "Epoch 19/25 | Train Loss: 0.000965 | Val Loss: 0.001103 | PSNR: 30.02 | SSIM: 0.8138\n",
      "Epoch 20/25 | Train Loss: 0.000961 | Val Loss: 0.000953 | PSNR: 30.86 | SSIM: 0.8564\n",
      "‚úÖ Best model saved at epoch 21 ‚Äî Val Loss: 0.000886\n",
      "Epoch 21/25 | Train Loss: 0.000966 | Val Loss: 0.000886 | PSNR: 31.15 | SSIM: 0.8713\n",
      "Epoch 22/25 | Train Loss: 0.000950 | Val Loss: 0.000904 | PSNR: 30.99 | SSIM: 0.8497\n",
      "Epoch 23/25 | Train Loss: 0.000933 | Val Loss: 0.000990 | PSNR: 30.39 | SSIM: 0.8100\n",
      "Epoch 24/25 | Train Loss: 0.000956 | Val Loss: 0.000908 | PSNR: 31.00 | SSIM: 0.8603\n",
      "Epoch 25/25 | Train Loss: 0.000912 | Val Loss: 0.000893 | PSNR: 31.05 | SSIM: 0.8525\n",
      "‚úÖ Final model saved to checkpoints/final_modelone.pth\n"
     ]
    }
   ],
   "source": [
    "# train.py\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "from skimage.metrics import peak_signal_noise_ratio, structural_similarity\n",
    "#from model import LSC_CNN\n",
    "#from dataset import PairedIRDataset\n",
    "\n",
    "def compute_metrics(pred, target):\n",
    "    pred_np = pred.cpu().numpy().squeeze()\n",
    "    target_np = target.cpu().numpy().squeeze()\n",
    "    psnr_vals, ssim_vals = [], []\n",
    "    for i in range(pred_np.shape[0]):\n",
    "        psnr_vals.append(peak_signal_noise_ratio(target_np[i], pred_np[i], data_range=1.0))\n",
    "        ssim_vals.append(structural_similarity(target_np[i], pred_np[i], data_range=1.0))\n",
    "    return np.mean(psnr_vals), np.mean(ssim_vals)\n",
    "\n",
    "# -----------------------------\n",
    "# ‚úÖ Updated Paths\n",
    "# -----------------------------\n",
    "train_clean_dir = \"/home/himanshu/Desktop/NUC_DATA/newfirstoriginal\"\n",
    "train_noisy_dir = \"/home/himanshu/Desktop/NUC_DATA/sigmareduced\"\n",
    "val_clean_dir   = \"/home/himanshu/Desktop/NUC_DATA/newfirstoriginal\"\n",
    "val_noisy_dir   = \"/home/himanshu/Desktop/NUC_DATA/newfirstnoise\"\n",
    "checkpoint_dir  = \"checkpoints\"\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 10      # 16\n",
    "num_epochs = 25\n",
    "lr = 1e-3\n",
    "crop_size = 50\n",
    "\n",
    "# Setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = LSC_CNN().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# Load data\n",
    "train_dataset = PairedIRDataset(train_clean_dir, train_noisy_dir, crop_size)\n",
    "val_dataset   = PairedIRDataset(val_clean_dir, val_noisy_dir, crop_size)\n",
    "train_loader  = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "val_loader    = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "\n",
    "# Training\n",
    "best_loss = float(\"inf\")\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for noisy, clean in train_loader:\n",
    "        noisy, clean = noisy.to(device), clean.to(device)\n",
    "        pred = model(noisy)\n",
    "        loss = loss_fn(pred, clean)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * noisy.size(0)\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss, psnr_total, ssim_total = 0.0, 0.0, 0.0\n",
    "    with torch.no_grad():\n",
    "        for noisy, clean in val_loader:\n",
    "            noisy, clean = noisy.to(device), clean.to(device)\n",
    "            pred = model(noisy)\n",
    "            val_loss += loss_fn(pred, clean).item() * noisy.size(0)\n",
    "            psnr, ssim = compute_metrics(pred, clean)\n",
    "            psnr_total += psnr * noisy.size(0)\n",
    "            ssim_total += ssim * noisy.size(0)\n",
    "\n",
    "    val_loss /= len(val_loader.dataset)\n",
    "    psnr_total /= len(val_loader.dataset)\n",
    "    ssim_total /= len(val_loader.dataset)\n",
    "\n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        torch.save(model.state_dict(), f\"{checkpoint_dir}/best_modelone.pth\")\n",
    "        print(f\"‚úÖ Best model saved at epoch {epoch+1} ‚Äî Val Loss: {val_loss:.6f}\")\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} | Train Loss: {train_loss:.6f} | Val Loss: {val_loss:.6f} | PSNR: {psnr_total:.2f} | SSIM: {ssim_total:.4f}\")\n",
    "\n",
    "torch.save(model.state_dict(), f\"{checkpoint_dir}/final_modelone.pth\")\n",
    "print(f\"‚úÖ Final model saved to {checkpoint_dir}/final_modelone.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "76b618d3-00ad-4a0e-ba97-510e4bc42461",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üñºÔ∏è Detected: Single image\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "calculate_psnr_ssim() missing 1 required positional argument: 'img2'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 159\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m input_path\u001b[38;5;241m.\u001b[39mlower()\u001b[38;5;241m.\u001b[39mendswith((\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.png\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.jpeg\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.bmp\u001b[39m\u001b[38;5;124m\"\u001b[39m)):\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124müñºÔ∏è Detected: Single image\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 159\u001b[0m     \u001b[43mprocess_single_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m input_path\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrealtime\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124müü¢ Realtime not supported with quality metrics.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[16], line 80\u001b[0m, in \u001b[0;36mprocess_single_image\u001b[0;34m(image_path)\u001b[0m\n\u001b[1;32m     77\u001b[0m rns_output \u001b[38;5;241m=\u001b[39m calculate_rns(output_tensor)\n\u001b[1;32m     79\u001b[0m \u001b[38;5;66;03m# PSNR and SSIM\u001b[39;00m\n\u001b[0;32m---> 80\u001b[0m psnr1, ssim1 \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_psnr_ssim\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_img_pil\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     81\u001b[0m psnr2, ssim2 \u001b[38;5;241m=\u001b[39m calculate_psnr_ssim(output_img_pi)\n\u001b[1;32m     83\u001b[0m \u001b[38;5;66;03m# Save\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: calculate_psnr_ssim() missing 1 required positional argument: 'img2'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from torchvision import transforms as T\n",
    "from PIL import Image\n",
    "from model import LSC_CNN  # Ensure your model is defined correctly\n",
    "from skimage.metrics import peak_signal_noise_ratio as compare_psnr\n",
    "from skimage.metrics import structural_similarity as compare_ssim\n",
    "\n",
    "# ----------------------------- #\n",
    "# Configuration\n",
    "# ----------------------------- #\n",
    "input_path = \"\"  # Change as needed\n",
    "output_dir = \"/home/himanshu/Desktop/NUC_DATA/nuc_dataset/dataset/test_data\"\n",
    "model_path = \"checkpoints/final_modelone.pth\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# ----------------------------- #\n",
    "# Load Model\n",
    "# ----------------------------- #\n",
    "model = LSC_CNN().to(device)\n",
    "model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "model.eval()\n",
    "\n",
    "# ----------------------------- #\n",
    "# Transforms\n",
    "# ----------------------------- #\n",
    "transform = T.ToTensor()\n",
    "to_pil = T.ToPILImage()\n",
    "\n",
    "# ----------------------------- #\n",
    "# Helper: RNS Calculation\n",
    "# ----------------------------- #\n",
    "def calculate_rns(tensor_img):\n",
    "    mean_val = torch.mean(tensor_img)\n",
    "    std_val = torch.std(tensor_img)\n",
    "    return (std_val / (mean_val + 1e-8)).item()\n",
    "\n",
    "# ----------------------------- #\n",
    "# PSNR and SSIM Calculation\n",
    "# ----------------------------- #\n",
    "def calculate_psnr_ssim(img1, img2):\n",
    "    img1_np = np.array(img1, dtype=np.float32) / 255.0\n",
    "    img2_np = np.array(img2, dtype=np.float32) / 255.0\n",
    "    psnr = compare_psnr(img1_np, img2_np, data_range=1.0)\n",
    "    ssim = compare_ssim(img1_np, img2_np, data_range=1.0)\n",
    "    return psnr, ssim\n",
    "\n",
    "# ----------------------------- #\n",
    "# Denoising Inference\n",
    "# ----------------------------- #\n",
    "def denoise_image(img: Image.Image):\n",
    "    img_gray = img.convert(\"L\")\n",
    "    img_tensor = transform(img_gray).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        output_tensor = model(img_tensor)\n",
    "    input_tensor = img_tensor.squeeze().cpu().clamp(0, 1)\n",
    "    output_tensor = output_tensor.squeeze().cpu().clamp(0, 1)\n",
    "    return input_tensor, output_tensor\n",
    "\n",
    "# ----------------------------- #\n",
    "# Process Single Image\n",
    "# ----------------------------- #\n",
    "def process_single_image(image_path):\n",
    "    img = Image.open(image_path).convert(\"L\")\n",
    "    input_tensor, output_tensor = denoise_image(img)\n",
    "\n",
    "    # Convert to PIL for metric calculation\n",
    "    input_img_pil = to_pil(input_tensor)\n",
    "    output_img_pil = to_pil(output_tensor)\n",
    "\n",
    "    # RNS\n",
    "    rns_input = calculate_rns(input_tensor)\n",
    "    rns_output = calculate_rns(output_tensor)\n",
    "\n",
    "    # PSNR and SSIM\n",
    "    psnr, ssim = calculate_psnr_ssim(input_img_pil, output_img_pil)output_img_pi\n",
    "\n",
    "    # Save\n",
    "    out_path = os.path.join(output_dir, os.path.basename(image_path))\n",
    "    output_img_pil.save(out_path)\n",
    "    output_img_pil.show(title=\"Denoised Image\")\n",
    "\n",
    "    # Display Results\n",
    "    print(\"\\nüìä Results (Image Quality Metrics):\")\n",
    "    print(\"---------------------------------------------------\")\n",
    "    print(f\"‚úÖ Saved denoised image: {out_path}\")\n",
    "    print(f\"üîπ RNS (Input Image):   {rns_input:.6f}\")\n",
    "    print(f\"üîπ RNS (Output Image):  {rns_output:.6f}\")\n",
    "    print(f\"üîπ PSNR:                {psnr:.2f} dB\")\n",
    "    print(f\"üîπ SSIM:                {ssim:.4f}\")\n",
    "    print(\"---------------------------------------------------\")\n",
    "\n",
    "# ----------------------------- #\n",
    "# Folder and Video Logic (Optional)\n",
    "# ----------------------------- #\n",
    "def process_folder(folder_path):\n",
    "    for fname in os.listdir(folder_path):\n",
    "        if fname.lower().endswith((\".jpg\", \".jpeg\", \".png\", \".bmp\")):\n",
    "            img_path = os.path.join(folder_path, fname)\n",
    "            process_single_image(img_path)\n",
    "\n",
    "def process_video(video_path):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "    out_path = os.path.join(output_dir, \"denoised_video.avi\")\n",
    "    out = None\n",
    "    frame_idx = 0\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        img = Image.fromarray(gray)\n",
    "        input_tensor, output_tensor = denoise_image(img)\n",
    "\n",
    "        input_img_pil = to_pil(input_tensor)\n",
    "        output_img_pil = to_pil(output_tensor)\n",
    "\n",
    "        rns_input = calculate_rns(input_tensor)\n",
    "        rns_output = calculate_rns(output_tensor)\n",
    "        psnr, ssim = calculate_psnr_ssim(input_img_pil, output_img_pil)\n",
    "\n",
    "        den_frame = np.array(output_img_pil)\n",
    "\n",
    "        if out is None:\n",
    "            h, w = den_frame.shape\n",
    "            out = cv2.VideoWriter(out_path, fourcc, 20.0, (w, h), isColor=False)\n",
    "\n",
    "        out.write(den_frame)\n",
    "\n",
    "        print(f\"üéûÔ∏è Frame {frame_idx:03d} | RNS_in: {rns_input:.4f} | RNS_out: {rns_output:.4f} | PSNR: {psnr:.2f} | SSIM: {ssim:.4f}\")\n",
    "        frame_idx += 1\n",
    "\n",
    "    cap.release()\n",
    "    if out:\n",
    "        out.release()\n",
    "        print(f\"\\n‚úÖ Denoised video saved to: {out_path}\")\n",
    "\n",
    "# ----------------------------- #\n",
    "# Main Entry\n",
    "# ----------------------------- #\n",
    "if os.path.isdir(input_path):\n",
    "    print(\"üìÅ Detected: Directory of images\")\n",
    "    process_folder(input_path)\n",
    "elif input_path.lower().endswith((\".mp4\", \".avi\", \".mov\", \".mkv\", \".mpg\", \".mpeg\")):\n",
    "    print(\"üéûÔ∏è Detected: Video file\")\n",
    "    process_video(input_path)\n",
    "elif input_path.lower().endswith((\".png\", \".jpg\", \".jpeg\", \".bmp\")):\n",
    "    print(\"üñºÔ∏è Detected: Single image\")\n",
    "    process_single_image(input_path)\n",
    "elif input_path.lower() == \"realtime\":\n",
    "    print(\"üü¢ Realtime not supported with quality metrics.\")\n",
    "else:\n",
    "    raise ValueError(\"‚ùå Unsupported input type.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "11e1bd04-8755-40b6-8de5-06b7461d5d33",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (744489226.py, line 80)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[29], line 80\u001b[0;36m\u001b[0m\n\u001b[0;31m    psnr, ssim = calculate_psnr_ssim(input_img_pil, output_img_pil)output_img_pi\u001b[0m\n\u001b[0m                                                                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from torchvision import transforms as T\n",
    "from PIL import Image\n",
    "from model import LSC_CNN  # Ensure your model is defined correctly\n",
    "from skimage.metrics import peak_signal_noise_ratio as compare_psnr\n",
    "from skimage.metrics import structural_similarity as compare_ssim\n",
    "\n",
    "# ----------------------------- #\n",
    "# Configuration\n",
    "# ----------------------------- #\n",
    "input_path = \"/home/himanshu/outputimage.jpg\"  # Change as needed\n",
    "output_dir = \"/home/himanshu/Desktop/NUC_DATA/nuc_dataset/dataset/test_data\"\n",
    "model_path = \"checkpoints/final_modelone.pth\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# ----------------------------- #\n",
    "# Load Model\n",
    "# ----------------------------- #\n",
    "model = LSC_CNN().to(device)\n",
    "model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "model.eval()\n",
    "\n",
    "# ----------------------------- #\n",
    "# Transforms\n",
    "# ----------------------------- #\n",
    "transform = T.ToTensor()\n",
    "to_pil = T.ToPILImage()\n",
    "\n",
    "# ----------------------------- #\n",
    "# Helper: RNS Calculation\n",
    "# ----------------------------- #\n",
    "def calculate_rns(tensor_img):\n",
    "    mean_val = torch.mean(tensor_img)\n",
    "    std_val = torch.std(tensor_img)\n",
    "    return (std_val / (mean_val + 1e-8)).item()\n",
    "\n",
    "# ----------------------------- #\n",
    "# PSNR and SSIM Calculation\n",
    "# ----------------------------- #\n",
    "def calculate_psnr_ssim(img1, img2):\n",
    "    img1_np = np.array(img1, dtype=np.float32) / 255.0\n",
    "    img2_np = np.array(img2, dtype=np.float32) / 255.0\n",
    "    psnr = compare_psnr(img1_np, data_range=1.0)\n",
    "    ssim = compare_ssim(img1_np, img2_np, data_range=1.0)\n",
    "    return psnr, ssim\n",
    "\n",
    "# ----------------------------- #\n",
    "# Denoising Inference\n",
    "# ----------------------------- #\n",
    "def denoise_image(img: Image.Image):\n",
    "    img_gray = img.convert(\"L\")\n",
    "    img_tensor = transform(img_gray).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        output_tensor = model(img_tensor)\n",
    "    input_tensor = img_tensor.squeeze().cpu().clamp(0, 1)\n",
    "    output_tensor = output_tensor.squeeze().cpu().clamp(0, 1)\n",
    "    return input_tensor, output_tensor\n",
    "\n",
    "# ----------------------------- #\n",
    "# Process Single Image\n",
    "# ----------------------------- #\n",
    "def process_single_image(image_path):\n",
    "    img = Image.open(image_path).convert(\"L\")\n",
    "    input_tensor, output_tensor = denoise_image(img)\n",
    "\n",
    "    # Convert to PIL for metric calculation\n",
    "    input_img_pil = to_pil(input_tensor)\n",
    "    output_img_pil = to_pil(output_tensor)\n",
    "\n",
    "    # RNS\n",
    "    rns_input = calculate_rns(input_tensor)\n",
    "    rns_output = calculate_rns(output_tensor)\n",
    "\n",
    "    # PSNR and SSIM\n",
    "    psnr, ssim = calculate_psnr_ssim(input_img_pil, output_img_pil)output_img_pi\n",
    "\n",
    "    # Save\n",
    "    out_path = os.path.join(output_dir, os.path.basename(image_path))\n",
    "    output_img_pil.save(out_path)\n",
    "    output_img_pil.show(title=\"Denoised Image\")\n",
    "\n",
    "    # Display Results\n",
    "    print(\"\\nüìä Results (Image Quality Metrics):\")\n",
    "    print(\"---------------------------------------------------\")\n",
    "    print(f\"‚úÖ Saved denoised image: {out_path}\")\n",
    "    print(f\"üîπ RNS (Input Image):   {rns_input:.6f}\")\n",
    "    print(f\"üîπ RNS (Output Image):  {rns_output:.6f}\")\n",
    "    print(f\"üîπ PSNR:                {psnr:.2f} dB\")\n",
    "    print(f\"üîπ SSIM:                {ssim:.4f}\")\n",
    "    print(\"---------------------------------------------------\")\n",
    "\n",
    "# ----------------------------- #\n",
    "# Folder and Video Logic (Optional)\n",
    "# ----------------------------- #\n",
    "def process_folder(folder_path):\n",
    "    for fname in os.listdir(folder_path):\n",
    "        if fname.lower().endswith((\".jpg\", \".jpeg\", \".png\", \".bmp\")):\n",
    "            img_path = os.path.join(folder_path, fname)\n",
    "            process_single_image(img_path)\n",
    "\n",
    "def process_video(video_path):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "    out_path = os.path.join(output_dir, \"denoised_video.avi\")\n",
    "    out = None\n",
    "    frame_idx = 0\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        img = Image.fromarray(gray)\n",
    "        input_tensor, output_tensor = denoise_image(img)\n",
    "\n",
    "        input_img_pil = to_pil(input_tensor)\n",
    "        output_img_pil = to_pil(output_tensor)\n",
    "\n",
    "        rns_input = calculate_rns(input_tensor)\n",
    "        rns_output = calculate_rns(output_tensor)\n",
    "        psnr, ssim = calculate_psnr_ssim(input_img_pil, output_img_pil)\n",
    "\n",
    "        den_frame = np.array(output_img_pil)\n",
    "\n",
    "        if out is None:\n",
    "            h, w = den_frame.shape\n",
    "            out = cv2.VideoWriter(out_path, fourcc, 20.0, (w, h), isColor=False)\n",
    "\n",
    "        out.write(den_frame)\n",
    "\n",
    "        print(f\"üéûÔ∏è Frame {frame_idx:03d} | RNS_in: {rns_input:.4f} | RNS_out: {rns_output:.4f} | PSNR: {psnr:.2f} | SSIM: {ssim:.4f}\")\n",
    "        frame_idx += 1\n",
    "\n",
    "    cap.release()\n",
    "    if out:\n",
    "        out.release()\n",
    "        print(f\"\\n‚úÖ Denoised video saved to: {out_path}\")\n",
    "\n",
    "# ----------------------------- #\n",
    "# Main Entry\n",
    "# ----------------------------- #\n",
    "if os.path.isdir(input_path):\n",
    "    print(\"üìÅ Detected: Directory of images\")\n",
    "    process_folder(input_path)\n",
    "elif input_path.lower().endswith((\".mp4\", \".avi\", \".mov\", \".mkv\", \".mpg\", \".mpeg\")):\n",
    "    print(\"üéûÔ∏è Detected: Video file\")\n",
    "    process_video(input_path)\n",
    "elif input_path.lower().endswith((\".png\", \".jpg\", \".jpeg\", \".bmp\")):\n",
    "    print(\"üñºÔ∏è Detected: Single image\")\n",
    "    process_single_image(input_path)\n",
    "elif input_path.lower() == \"realtime\":\n",
    "    print(\"üü¢ Realtime not supported with quality metrics.\")\n",
    "else:\n",
    "    raise ValueError(\"‚ùå Unsupported input type.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b9591bbd-ddaf-41c1-b239-fae40ccf6606",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üñºÔ∏è Detected: Single image\n",
      "\n",
      "üìä Results (Image Quality Metrics):\n",
      "------------------------------------------------------------\n",
      "Metric          | Input Image     | Denoised Image \n",
      "------------------------------------------------------------\n",
      "RNS             | 1.274183        | 1.337069       \n",
      "PSNR (dB)       | inf             | 24.80          \n",
      "SSIM            | 1.0000          | 0.3748         \n",
      "------------------------------------------------------------\n",
      "‚úÖ Saved denoised image: /home/himanshu/Desktop/NUC_DATA/nuc_dataset/dataset/test_data/outputimage.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/himanshu/.local/lib/python3.10/site-packages/torch/nn/modules/conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at /opt/pytorch/aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
      "  return F.conv2d(input, weight, bias, self.stride,\n",
      "/home/himanshu/.local/lib/python3.10/site-packages/skimage/metrics/simple_metrics.py:168: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  return 10 * np.log10((data_range**2) / err)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from torchvision import transforms as T\n",
    "from PIL import Image\n",
    "#from model import LSC_CNN  # Ensure your model is defined correctly\n",
    "from skimage.metrics import peak_signal_noise_ratio as compare_psnr\n",
    "from skimage.metrics import structural_similarity as compare_ssim\n",
    "\n",
    "# ----------------------------- #\n",
    "# Configuration\n",
    "# ----------------------------- #\n",
    "input_path = \"/home/himanshu/outputimage.jpg\"  # Change as needed\n",
    "output_dir = \"/home/himanshu/Desktop/NUC_DATA/nuc_dataset/dataset/test_data\"\n",
    "model_path = \"checkpoints/final_modelone.pth\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# ----------------------------- #\n",
    "# Load Model\n",
    "# ----------------------------- #\n",
    "model = LSC_CNN().to(device)\n",
    "model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "model.eval()\n",
    "\n",
    "# ----------------------------- #\n",
    "# Transforms\n",
    "# ----------------------------- #\n",
    "transform = T.ToTensor()\n",
    "to_pil = T.ToPILImage()\n",
    "\n",
    "# ----------------------------- #\n",
    "# Helper: RNS Calculation\n",
    "# ----------------------------- #\n",
    "def calculate_rns(tensor_img):\n",
    "    mean_val = torch.mean(tensor_img)\n",
    "    std_val = torch.std(tensor_img)\n",
    "    return (std_val / (mean_val + 1e-8)).item()\n",
    "\n",
    "# ----------------------------- #\n",
    "# PSNR and SSIM Calculation\n",
    "# ----------------------------- #\n",
    "def calculate_psnr_ssim(img1, img2):\n",
    "    img1_np = np.array(img1, dtype=np.float32) / 255.0\n",
    "    img2_np = np.array(img2, dtype=np.float32) / 255.0\n",
    "    psnr = compare_psnr(img1_np, img2_np, data_range=1.0)\n",
    "    ssim = compare_ssim(img1_np, img2_np, data_range=1.0)\n",
    "    return psnr, ssim\n",
    "\n",
    "# ----------------------------- #\n",
    "# Denoising Inference\n",
    "# ----------------------------- #\n",
    "def denoise_image(img: Image.Image):\n",
    "    img_gray = img.convert(\"L\")\n",
    "    img_tensor = transform(img_gray).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        output_tensor = model(img_tensor)\n",
    "    input_tensor = img_tensor.squeeze().cpu().clamp(0, 1)\n",
    "    output_tensor = output_tensor.squeeze().cpu().clamp(0, 1)\n",
    "    return input_tensor, output_tensor\n",
    "\n",
    "# ----------------------------- #\n",
    "# Process Single Image\n",
    "# ----------------------------- #\n",
    "def process_single_image(image_path):\n",
    "    img = Image.open(image_path).convert(\"L\")\n",
    "    input_tensor, output_tensor = denoise_image(img)\n",
    "\n",
    "    # Convert to PIL for metric calculation\n",
    "    input_img_pil = to_pil(input_tensor)\n",
    "    output_img_pil = to_pil(output_tensor)\n",
    "\n",
    "    # RNS\n",
    "    rns_input = calculate_rns(input_tensor)\n",
    "    rns_output = calculate_rns(output_tensor)\n",
    "\n",
    "    # PSNR and SSIM\n",
    "    psnr_input, ssim_input = calculate_psnr_ssim(input_img_pil, input_img_pil)\n",
    "    psnr_output, ssim_output = calculate_psnr_ssim(input_img_pil, output_img_pil)\n",
    "\n",
    "    # Save and Show Output Image\n",
    "    out_path = os.path.join(output_dir, os.path.basename(image_path))\n",
    "    output_img_pil.save(out_path)\n",
    "    output_img_pil.show(title=\"Denoised Image\")\n",
    "\n",
    "    # Display Results in Table Format\n",
    "    print(\"\\nüìä Results (Image Quality Metrics):\")\n",
    "    print(\"------------------------------------------------------------\")\n",
    "    print(f\"{'Metric':<15} | {'Input Image':<15} | {'Denoised Image':<15}\")\n",
    "    print(\"------------------------------------------------------------\")\n",
    "    print(f\"{'RNS':<15} | {rns_input:<15.6f} | {rns_output:<15.6f}\")\n",
    "    print(f\"{'PSNR (dB)':<15} | {psnr_input:<15.2f} | {psnr_output:<15.2f}\")\n",
    "    print(f\"{'SSIM':<15} | {ssim_input:<15.4f} | {ssim_output:<15.4f}\")\n",
    "    print(\"------------------------------------------------------------\")\n",
    "    print(f\"‚úÖ Saved denoised image: {out_path}\")\n",
    "\n",
    "# ----------------------------- #\n",
    "# Folder and Video Logic (Optional)\n",
    "# ----------------------------- #\n",
    "def process_folder(folder_path):\n",
    "    for fname in os.listdir(folder_path):\n",
    "        if fname.lower().endswith((\".jpg\", \".jpeg\", \".png\", \".bmp\")):\n",
    "            img_path = os.path.join(folder_path, fname)\n",
    "            process_single_image(img_path)\n",
    "\n",
    "def process_video(video_path):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "    out_path = os.path.join(output_dir, \"denoised_video.avi\")\n",
    "    out = None\n",
    "    frame_idx = 0\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        img = Image.fromarray(gray)\n",
    "        input_tensor, output_tensor = denoise_image(img)\n",
    "\n",
    "        input_img_pil = to_pil(input_tensor)\n",
    "        output_img_pil = to_pil(output_tensor)\n",
    "\n",
    "        rns_input = calculate_rns(input_tensor)\n",
    "        rns_output = calculate_rns(output_tensor)\n",
    "        psnr_output, ssim_output = calculate_psnr_ssim(input_img_pil, output_img_pil)\n",
    "\n",
    "        den_frame = np.array(output_img_pil)\n",
    "\n",
    "        if out is None:\n",
    "            h, w = den_frame.shape\n",
    "            out = cv2.VideoWriter(out_path, fourcc, 20.0, (w, h), isColor=False)\n",
    "\n",
    "        out.write(den_frame)\n",
    "\n",
    "        print(f\"üéûÔ∏è Frame {frame_idx:03d} | RNS_in: {rns_input:.4f} | RNS_out: {rns_output:.4f} | PSNR: {psnr_output:.2f} | SSIM: {ssim_output:.4f}\")\n",
    "        frame_idx += 1\n",
    "\n",
    "    cap.release()\n",
    "    if out:\n",
    "        out.release()\n",
    "        print(f\"\\n‚úÖ Denoised video saved to: {out_path}\")\n",
    "\n",
    "# ----------------------------- #\n",
    "# Main Entry\n",
    "# ----------------------------- #\n",
    "if os.path.isdir(input_path):\n",
    "    print(\"üìÅ Detected: Directory of images\")\n",
    "    process_folder(input_path)\n",
    "elif input_path.lower().endswith((\".mp4\", \".avi\", \".mov\", \".mkv\", \".mpg\", \".mpeg\")):\n",
    "    print(\"üéûÔ∏è Detected: Video file\")\n",
    "    process_video(input_path)\n",
    "elif input_path.lower().endswith((\".png\", \".jpg\", \".jpeg\", \".bmp\")):\n",
    "    print(\"üñºÔ∏è Detected: Single image\")\n",
    "    process_single_image(input_path)\n",
    "elif input_path.lower() == \"realtime\":\n",
    "    print(\"üü¢ Realtime not supported with quality metrics.\")\n",
    "else:\n",
    "    raise ValueError(\"‚ùå Unsupported input type.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83147006-ea50-4046-a1c5-aa36e20eb5df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
